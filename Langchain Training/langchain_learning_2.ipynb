{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nSplash Socks'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.predict(\"What would be a good company name for a company that makes colorful socks?\")\n",
    "# >> Feetful of Fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"J'aime programmer.\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "\n",
    "chat = ChatOpenAI(temperature=0)\n",
    "chat.predict_messages([HumanMessage(content=\"Translate this sentence from English to French. I love programming.\")])\n",
    "# >> AIMessage(content=\"J'aime programmer.\", additional_kwargs={})\n",
    "\n",
    "chat.predict(\"Translate this sentence from English to French. I love programming.\")\n",
    "# >> J'aime programmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is a good name for a company that makes colorful socks?'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"What is a good name for a company that makes {product}?\")\n",
    "prompt.format(product=\"colorful socks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nCozyKicks.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "chain.run(\"colorful socks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant that translates English to French.', additional_kwargs={}),\n",
       " HumanMessage(content='I love programming.', additional_kwargs={}, example=False)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "template = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "human_template = \"{text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "chat_prompt.format_messages(input_language=\"English\", output_language=\"French\", text=\"I love programming.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant that translates English to French.', additional_kwargs={}),\n",
       " HumanMessage(content='I love programming.', additional_kwargs={}, example=False)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\n",
    "    SystemMessage(content=\"You are a helpful assistant that translates English to French.\", additional_kwargs={}),\n",
    "    HumanMessage(content=\"I love programming.\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"J'adore la programmation.\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = LLMChain(llm=chat, prompt=chat_prompt)\n",
    "chain.run(input_language=\"English\", output_language=\"French\", text=\"I love programming.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find the temperature first, then use the calculator to raise it to the .023 power.\n",
      "Action: Search\n",
      "Action Input: \"High temperature in SF yesterday\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mHigh: 66.2ºf @1:40 PM Low: 55.04ºf @5:56 AM Approx. Precipitation / Rain Total: in. 1hr.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now need to use the calculator to raise 66.2 to the .023 power\n",
      "Action: Calculator\n",
      "Action Input: 66.2^.023\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 1.1012343099196273\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: 1.1012343099196273\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.1012343099196273'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from langchain.agents import AgentType, initialize_agent, load_tools\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "import os\n",
    "os.environ[\"SERPAPI_API_KEY\"] = 'cd0b32653a536c8636cb64d0cd099fef7abd7d4dcb4123cd70d0226342df407b'\n",
    "\n",
    "# The language model we're going to use to control the agent.\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "# The tools we'll give the Agent access to. Note that the 'llm-math' tool uses an LLM, so we need to pass that in.\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
    "\n",
    "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "\n",
    "# Let's test it out!\n",
    "agent.run(\"What was the high temperature in SF yesterday in Fahrenheit? What is that number raised to the .023 power?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi there!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Hi there! It's nice to meet you. My name is AI. What's your name?\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import OpenAI, ConversationChain\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "conversation = ConversationChain(llm=llm, verbose=True)\n",
    "\n",
    "conversation.run(\"Hi there!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate\n",
    ")\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(\n",
    "        \"The following is a friendly conversation between a human and an AI. The AI is talkative and \"\n",
    "        \"provides lots of specific details from its context. If the AI does not know the answer to a \"\n",
    "        \"question, it truthfully says it does not know.\"\n",
    "    ),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{input}\")\n",
    "])\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "conversation = ConversationChain(memory=memory, prompt=prompt, llm=llm)\n",
    "\n",
    "conversation.predict(input=\"Hi there!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"That sounds like fun! I'm happy to chat with you. Is there anything specific you'd like to talk about?\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Sure! I am an AI language model created by OpenAI. I was trained on a large dataset of text from the internet, which allows me to understand and generate human-like language. I can answer questions, provide information, and even have conversations like this one. Is there anything else you'd like to know about me?\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Tell me about yourself.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next set of cells has to do with Modules that are interfaces and external integrations.  The first is Model Input and Output which interface with language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are a naming consultant for new companies. What is a good name for a company that makes colorful socks?'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "\n",
    "template = \"\"\"You are a naming consultant for new companies. What is a good name for a company that makes {product}?\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "prompt.format(product=\"colorful socks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tell me a funny joke about chickens.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "# An example prompt with no input variables\n",
    "no_input_prompt = PromptTemplate(input_variables=[], template=\"Tell me a joke.\")\n",
    "no_input_prompt.format()\n",
    "# -> \"Tell me a joke.\"\n",
    "\n",
    "# An example prompt with one input variable\n",
    "one_input_prompt = PromptTemplate(input_variables=[\"adjective\"], template=\"Tell me a {adjective} joke.\")\n",
    "one_input_prompt.format(adjective=\"funny\")\n",
    "# -> \"Tell me a funny joke.\"\n",
    "\n",
    "# An example prompt with multiple input variables\n",
    "multiple_input_prompt = PromptTemplate(\n",
    "    input_variables=[\"adjective\", \"content\"], \n",
    "    template=\"Tell me a {adjective} joke about {content}.\"\n",
    ")\n",
    "multiple_input_prompt.format(adjective=\"funny\", content=\"chickens\")\n",
    "# -> \"Tell me a funny joke about chickens.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tell me a funny joke about chickens.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"Tell me a {adjective} joke about {content}.\"\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(template)\n",
    "prompt_template.input_variables\n",
    "# -> ['adjective', 'content']\n",
    "prompt_template.format(adjective=\"funny\", content=\"chickens\")\n",
    "# -> Tell me a funny joke about chickens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    PromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "template=\"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "human_template=\"{text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=PromptTemplate(\n",
    "    template=\"You are a helpful assistant that translates {input_language} to {output_language}.\",\n",
    "    input_variables=[\"input_language\", \"output_language\"],\n",
    ")\n",
    "system_message_prompt_2 = SystemMessagePromptTemplate(prompt=prompt)\n",
    "\n",
    "assert system_message_prompt == system_message_prompt_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant that translates English to French.', additional_kwargs={}),\n",
       " HumanMessage(content='I love programming.', additional_kwargs={}, example=False)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "# get a chat completion from the formatted messages\n",
    "chat_prompt.format_prompt(input_language=\"English\", output_language=\"French\", text=\"I love programming.\").to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feast \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key:  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\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = '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'\n",
    "print('OpenAI API Key: ', os.environ[\"OPENAI_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_timestamp</th>\n",
       "      <th>driver_id</th>\n",
       "      <th>conv_rate</th>\n",
       "      <th>acc_rate</th>\n",
       "      <th>avg_daily_trips</th>\n",
       "      <th>created</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-06-20 20:29:43.289666+00:00</td>\n",
       "      <td>1001</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1000</td>\n",
       "      <td>2023-06-20 20:29:43.289666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-06-20 20:26:25.198826+00:00</td>\n",
       "      <td>1001</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1000</td>\n",
       "      <td>2023-06-20 20:26:25.198826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-06-05 19:00:00+00:00</td>\n",
       "      <td>1005</td>\n",
       "      <td>0.218041</td>\n",
       "      <td>0.244001</td>\n",
       "      <td>60</td>\n",
       "      <td>2023-06-20 19:52:21.161000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-06-05 20:00:00+00:00</td>\n",
       "      <td>1005</td>\n",
       "      <td>0.199407</td>\n",
       "      <td>0.965895</td>\n",
       "      <td>393</td>\n",
       "      <td>2023-06-20 19:52:21.161000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-06-05 21:00:00+00:00</td>\n",
       "      <td>1005</td>\n",
       "      <td>0.321204</td>\n",
       "      <td>0.260528</td>\n",
       "      <td>9</td>\n",
       "      <td>2023-06-20 19:52:21.161000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1804</th>\n",
       "      <td>2023-06-20 17:00:00+00:00</td>\n",
       "      <td>1001</td>\n",
       "      <td>0.730574</td>\n",
       "      <td>0.217883</td>\n",
       "      <td>435</td>\n",
       "      <td>2023-06-20 19:52:21.161000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1805</th>\n",
       "      <td>2023-06-20 18:00:00+00:00</td>\n",
       "      <td>1001</td>\n",
       "      <td>0.839837</td>\n",
       "      <td>0.865881</td>\n",
       "      <td>731</td>\n",
       "      <td>2023-06-20 19:52:21.161000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1806</th>\n",
       "      <td>2021-04-12 07:00:00+00:00</td>\n",
       "      <td>1001</td>\n",
       "      <td>0.766323</td>\n",
       "      <td>0.480534</td>\n",
       "      <td>594</td>\n",
       "      <td>2023-06-20 19:52:21.161000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1807</th>\n",
       "      <td>2023-06-13 07:00:00+00:00</td>\n",
       "      <td>1003</td>\n",
       "      <td>0.147717</td>\n",
       "      <td>0.857229</td>\n",
       "      <td>389</td>\n",
       "      <td>2023-06-20 19:52:21.161000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1808</th>\n",
       "      <td>2023-06-13 07:00:00+00:00</td>\n",
       "      <td>1003</td>\n",
       "      <td>0.147717</td>\n",
       "      <td>0.857229</td>\n",
       "      <td>389</td>\n",
       "      <td>2023-06-20 19:52:21.161000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1809 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      event_timestamp  driver_id  conv_rate  acc_rate  \\\n",
       "0    2023-06-20 20:29:43.289666+00:00       1001   1.000000  1.000000   \n",
       "1    2023-06-20 20:26:25.198826+00:00       1001   1.000000  1.000000   \n",
       "2           2023-06-05 19:00:00+00:00       1005   0.218041  0.244001   \n",
       "3           2023-06-05 20:00:00+00:00       1005   0.199407  0.965895   \n",
       "4           2023-06-05 21:00:00+00:00       1005   0.321204  0.260528   \n",
       "...                               ...        ...        ...       ...   \n",
       "1804        2023-06-20 17:00:00+00:00       1001   0.730574  0.217883   \n",
       "1805        2023-06-20 18:00:00+00:00       1001   0.839837  0.865881   \n",
       "1806        2021-04-12 07:00:00+00:00       1001   0.766323  0.480534   \n",
       "1807        2023-06-13 07:00:00+00:00       1003   0.147717  0.857229   \n",
       "1808        2023-06-13 07:00:00+00:00       1003   0.147717  0.857229   \n",
       "\n",
       "      avg_daily_trips                    created  \n",
       "0                1000 2023-06-20 20:29:43.289666  \n",
       "1                1000 2023-06-20 20:26:25.198826  \n",
       "2                  60 2023-06-20 19:52:21.161000  \n",
       "3                 393 2023-06-20 19:52:21.161000  \n",
       "4                   9 2023-06-20 19:52:21.161000  \n",
       "...               ...                        ...  \n",
       "1804              435 2023-06-20 19:52:21.161000  \n",
       "1805              731 2023-06-20 19:52:21.161000  \n",
       "1806              594 2023-06-20 19:52:21.161000  \n",
       "1807              389 2023-06-20 19:52:21.161000  \n",
       "1808              389 2023-06-20 19:52:21.161000  \n",
       "\n",
       "[1809 rows x 6 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "pd.read_parquet(\"./feature_repo/data/driver_stats.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feast import FeatureStore\n",
    "\n",
    "# You may need to update the path depending on where you stored it\n",
    "feast_repo_path = \"./feature_repo/\"\n",
    "store = FeatureStore(repo_path=feast_repo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate, StringPromptTemplate\n",
    "\n",
    "template = \"\"\"Given the driver's up to date stats, write them note relaying those stats to them.\n",
    "If they have a conversation rate above .5, give them a compliment. Otherwise, make a silly joke about chickens at the end to make them feel better\n",
    "\n",
    "Here are the drivers stats:\n",
    "Conversation rate: {conv_rate}\n",
    "Acceptance rate: {acc_rate}\n",
    "Average Daily Trips: {avg_daily_trips}\n",
    "\n",
    "Your response:\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeastPromptTemplate(StringPromptTemplate):\n",
    "    def format(self, **kwargs) -> str:\n",
    "        driver_id = kwargs.pop(\"driver_id\")\n",
    "        feature_vector = store.get_online_features(\n",
    "            features=[\n",
    "                \"driver_hourly_stats:conv_rate\",\n",
    "                \"driver_hourly_stats:acc_rate\",\n",
    "                \"driver_hourly_stats:avg_daily_trips\",\n",
    "            ],\n",
    "            entity_rows=[{\"driver_id\": driver_id}],\n",
    "        ).to_dict()\n",
    "        kwargs[\"conv_rate\"] = feature_vector[\"conv_rate\"][0]\n",
    "        kwargs[\"acc_rate\"] = feature_vector[\"acc_rate\"][0]\n",
    "        kwargs[\"avg_daily_trips\"] = feature_vector[\"avg_daily_trips\"][0]\n",
    "        return prompt.format(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = FeastPromptTemplate(input_variables=[\"driver_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given the driver's up to date stats, write them note relaying those stats to them.\n",
      "If they have a conversation rate above .5, give them a compliment. Otherwise, make a silly joke about chickens at the end to make them feel better\n",
      "\n",
      "Here are the drivers stats:\n",
      "Conversation rate: 0.12425403296947479\n",
      "Acceptance rate: 0.13909593224525452\n",
      "Average Daily Trips: 388\n",
      "\n",
      "Your response:\n"
     ]
    }
   ],
   "source": [
    "print(prompt_template.format(driver_id=1002))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello there! Just wanted to let you know that your current stats are looking good. Your conversation rate may not be over .5, but don't worry, that just means you have room to improve! And hey, did you hear about the chicken who could drive? I heard he was an egg-cellent driver. Keep up the good work!\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(llm=ChatOpenAI(), prompt=prompt_template)\n",
    "\n",
    "chain.run(1002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Given the function name and source code, generate an English language explanation of the function.\n",
      "        Function Name: get_source_code\n",
      "        Source Code:\n",
      "        def get_source_code(function_name):\n",
      "    # Get the source code of the function\n",
      "    return inspect.getsource(function_name)\n",
      "\n",
      "        Explanation:\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "\n",
    "\n",
    "def get_source_code(function_name):\n",
    "    # Get the source code of the function\n",
    "    return inspect.getsource(function_name)\n",
    "\n",
    "from langchain.prompts import StringPromptTemplate\n",
    "from pydantic import BaseModel, validator\n",
    "\n",
    "\n",
    "class FunctionExplainerPromptTemplate(StringPromptTemplate, BaseModel):\n",
    "    \"\"\"A custom prompt template that takes in the function name as input, and formats the prompt template to provide the source code of the function.\"\"\"\n",
    "\n",
    "    @validator(\"input_variables\")\n",
    "    def validate_input_variables(cls, v):\n",
    "        \"\"\"Validate that the input variables are correct.\"\"\"\n",
    "        if len(v) != 1 or \"function_name\" not in v:\n",
    "            raise ValueError(\"function_name must be the only input_variable.\")\n",
    "        return v\n",
    "\n",
    "    def format(self, **kwargs) -> str:\n",
    "        # Get the source code of the function\n",
    "        source_code = get_source_code(kwargs[\"function_name\"])\n",
    "\n",
    "        # Generate the prompt to be sent to the language model\n",
    "        prompt = f\"\"\"\n",
    "        Given the function name and source code, generate an English language explanation of the function.\n",
    "        Function Name: {kwargs[\"function_name\"].__name__}\n",
    "        Source Code:\n",
    "        {source_code}\n",
    "        Explanation:\n",
    "        \"\"\"\n",
    "        return prompt\n",
    "\n",
    "    def _prompt_type(self):\n",
    "        return \"function-explainer\"\n",
    "\n",
    "\n",
    "fn_explainer = FunctionExplainerPromptTemplate(input_variables=[\"function_name\"])\n",
    "\n",
    "# Generate a prompt for the function \"get_source_code\"\n",
    "prompt = fn_explainer.format(function_name=get_source_code)\n",
    "print(prompt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "OPENAI_API_KEY=\"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\"\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "openai api models.list\n",
    "\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(client= davinci, openai_api_key=OPENAI_API_KEY, temperature=0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openailearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
